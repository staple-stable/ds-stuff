{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных в датафрейм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_DATA = './test/'\n",
    "PATH_TO_TRAIN_DATA = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_set(path_to_files):\n",
    "    df = pd.DataFrame(columns=['text', 'target'])\n",
    "    positive_files = glob(path_to_files + \"pos/*.txt\")\n",
    "    negative_files = glob(path_to_files + \"neg/*.txt\")\n",
    "    \n",
    "    for file in tqdm(positive_files):\n",
    "        with open(file, 'r', encoding=\"utf8\") as content_file:\n",
    "            content = content_file.read()\n",
    "            df.loc[df.shape[0]] = [content, 1]\n",
    "    for file in tqdm(negative_files):\n",
    "        with open(file, 'r', encoding=\"utf8\") as content_file:\n",
    "            content = content_file.read()\n",
    "            df.loc[df.shape[0]] = [content, 0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e16a72da57d4880825b94a55b2885cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e46e4cc4824dbb9be9cb4b0ecbd99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = prepare_train_set(PATH_TO_TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сериализуем датафрейм, чтобы не пересчитывать при перезапуске ядра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./pickle/df.pkl\", 'wb') as df_pkl:\n",
    "    pickle.dump(df, df_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"./pickle/df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my  years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homelessness or houselessness as george carlin stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq pressuring kids to succeed technology the elections inflation or worrying if theyll be next to end up on the streetsbr br but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what its like to be homeless that is goddard bolts lessonbr br mel brooks who directs who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival jeffery tambor to see if he can live in the streets for thirty days without the luxuries if bolt succeeds he can do what he wants with a future project of making more buildings the bets on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he cant step off the sidewalk hes given the nickname pepto by a vagrant after its written on his forehead where bolt meets other characters including a woman by the name of molly lesley ann warren an exdancer who got divorce before losing her home and her pals sailor howard morris and fumes teddy wilson who are already used to the streets theyre survivors bolt isnt hes not used to reaching mutual agreements like he once did when being rich where its fight or flight kill or be killedbr br while the love connection between molly and bolt wasnt necessary to plot i found life stinks to be one of mel brooks observant films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing saddles young frankenstein or spaceballs for the matter to show what its like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they dont know what to do with their money maybe they should give it to the homeless instead of using it like monopoly moneybr br or maybe this film will inspire you to help others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brilliant overacting by lesley ann warren best dramatic hobo lady i have ever seen and love scenes in clothes warehouse are second to none the corn on face is a classic as good as anything in blazing saddles the take on lawyers is also superb after being accused of being a turncoat selling out his boss and being dishonest the lawyer of pepto bolt shrugs indifferently im a lawyer he says three funny words jeffrey tambor a favorite from the later larry sanders show is fantastic here too as a mad millionaire who wants to crush the ghetto his character is more malevolent than usual the hospital scene and the scene where the homeless invade a demolition site are alltime classics look for the legs scene and the two big diggers fighting one bleeds this movie gets better each time i see it which is quite often</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "0  bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my  years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1  homelessness or houselessness as george carlin stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq pressuring kids to succeed technology the elections inflation or worrying if theyll be next to end up on the streetsbr br but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what its like to be homeless that is goddard bolts lessonbr br mel brooks who directs who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival jeffery tambor to see if he can live in the streets for thirty days without the luxuries if bolt succeeds he can do what he wants with a future project of making more buildings the bets on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he cant step off the sidewalk hes given the nickname pepto by a vagrant after its written on his forehead where bolt meets other characters including a woman by the name of molly lesley ann warren an exdancer who got divorce before losing her home and her pals sailor howard morris and fumes teddy wilson who are already used to the streets theyre survivors bolt isnt hes not used to reaching mutual agreements like he once did when being rich where its fight or flight kill or be killedbr br while the love connection between molly and bolt wasnt necessary to plot i found life stinks to be one of mel brooks observant films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing saddles young frankenstein or spaceballs for the matter to show what its like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they dont know what to do with their money maybe they should give it to the homeless instead of using it like monopoly moneybr br or maybe this film will inspire you to help others   \n",
       "2  brilliant overacting by lesley ann warren best dramatic hobo lady i have ever seen and love scenes in clothes warehouse are second to none the corn on face is a classic as good as anything in blazing saddles the take on lawyers is also superb after being accused of being a turncoat selling out his boss and being dishonest the lawyer of pepto bolt shrugs indifferently im a lawyer he says three funny words jeffrey tambor a favorite from the later larry sanders show is fantastic here too as a mad millionaire who wants to crush the ghetto his character is more malevolent than usual the hospital scene and the scene where the homeless invade a demolition site are alltime classics look for the legs scene and the two big diggers fighting one bleeds this movie gets better each time i see it which is quite often                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "  target  \n",
       "0  1      \n",
       "1  1      \n",
       "2  1      "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df.text = df.text.apply(lambda x: x.lower())\n",
    "\n",
    "tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "df.text = df.text.apply(lambda x: x.translate(tr))\n",
    "df.text = df.text.str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my  years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "0  bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my  years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt   \n",
       "\n",
       "  target  \n",
       "0  1      "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stoplist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homelessness houselessness george carlin stated issue years never plan help street considered human everything going school work vote matter people think homeless lost cause worrying things racism war iraq pressuring kids succeed technology elections inflation worrying theyll next end streetsbr br given bet live streets month without luxuries home entertainment sets bathroom pictures wall computer everything treasure see like homeless goddard bolts lessonbr br mel brooks directs stars bolt plays rich man everything world deciding make bet sissy rival jeffery tambor see live streets thirty days without luxuries bolt succeeds wants future project making buildings bets bolt thrown street bracelet leg monitor every move cant step sidewalk hes given nickname pepto vagrant written forehead bolt meets characters including woman name molly lesley ann warren exdancer got divorce losing home pals sailor howard morris fumes teddy wilson already used streets theyre survivors bolt isnt hes used reaching mutual agreements like rich fight flight kill killedbr br love connection molly bolt wasnt necessary plot found life stinks one mel brooks observant films prior comedy shows tender side compared slapstick work blazing saddles young frankenstein spaceballs matter show like something valuable losing next day hand making stupid bet like rich people dont know money maybe give homeless instead using like monopoly moneybr br maybe film inspire help others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brilliant overacting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse second none corn face classic good anything blazing saddles take lawyers also superb accused turncoat selling boss dishonest lawyer pepto bolt shrugs indifferently im lawyer says three funny words jeffrey tambor favorite later larry sanders show fantastic mad millionaire wants crush ghetto character malevolent usual hospital scene scene homeless invade demolition site alltime classics look legs scene two big diggers fighting one bleeds movie gets better time see quite often</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1  homelessness houselessness george carlin stated issue years never plan help street considered human everything going school work vote matter people think homeless lost cause worrying things racism war iraq pressuring kids succeed technology elections inflation worrying theyll next end streetsbr br given bet live streets month without luxuries home entertainment sets bathroom pictures wall computer everything treasure see like homeless goddard bolts lessonbr br mel brooks directs stars bolt plays rich man everything world deciding make bet sissy rival jeffery tambor see live streets thirty days without luxuries bolt succeeds wants future project making buildings bets bolt thrown street bracelet leg monitor every move cant step sidewalk hes given nickname pepto vagrant written forehead bolt meets characters including woman name molly lesley ann warren exdancer got divorce losing home pals sailor howard morris fumes teddy wilson already used streets theyre survivors bolt isnt hes used reaching mutual agreements like rich fight flight kill killedbr br love connection molly bolt wasnt necessary plot found life stinks one mel brooks observant films prior comedy shows tender side compared slapstick work blazing saddles young frankenstein spaceballs matter show like something valuable losing next day hand making stupid bet like rich people dont know money maybe give homeless instead using like monopoly moneybr br maybe film inspire help others   \n",
       "2  brilliant overacting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse second none corn face classic good anything blazing saddles take lawyers also superb accused turncoat selling boss dishonest lawyer pepto bolt shrugs indifferently im lawyer says three funny words jeffrey tambor favorite later larry sanders show fantastic mad millionaire wants crush ghetto character malevolent usual hospital scene scene homeless invade demolition site alltime classics look legs scene two big diggers fighting one bleeds movie gets better time see quite often                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "  target  \n",
       "0  1      \n",
       "1  1      \n",
       "2  1      "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим PorterStemer на входные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "df['stem_text'] = df['text'].apply(stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также применим лемматизацию в виде отдельного признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = sentence.split()\n",
    "    lemmed_tokens = [lemmatizer.lemmatize(token, get_pos(token)) for token in tokens]\n",
    "    return ' '.join(lemmed_tokens)\n",
    "\n",
    "df['lemm_text'] = df['text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwel high cartoon comedi ran time program school life teacher year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt</td>\n",
       "      <td>bromwell high cartoon comedy run time program school life teacher year teaching profession lead believe bromwell high satire much close reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school know student saw episode student repeatedly try burn school immediately recall high classic line inspector im sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetch pity isnt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0  bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt   \n",
       "\n",
       "  target  \\\n",
       "0  1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                               stem_text  \\\n",
       "0  bromwel high cartoon comedi ran time program school life teacher year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    lemm_text  \n",
       "0  bromwell high cartoon comedy run time program school life teacher year teaching profession lead believe bromwell high satire much close reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school know student saw episode student repeatedly try burn school immediately recall high classic line inspector im sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetch pity isnt  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим аналогичные операции для отложенной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e48b405aac040e3a03c020c19ea2163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ef712714bb4e34b579ea52d46ba8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = prepare_train_set(PATH_TO_TEST_DATA)\n",
    "test_df.text = test_df.text.apply(lambda x: x.lower())\n",
    "test_df.text = test_df.text.apply(lambda x: x.translate(tr))\n",
    "test_df.text = test_df.text.str.replace('\\d+', '')\n",
    "test_df.text = test_df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stoplist]))\n",
    "test_df['stem_text'] = test_df['text'].apply(stem_sentence)\n",
    "test_df['lemm_text'] = test_df['text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим bag-of-words для получения фич по токенам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(vectorizer, model, n = 10):\n",
    "    coefs_dict = dict(zip(vectorizer.get_feature_names(), model.coef_[0]))\n",
    "    sorted_dict = sorted(coefs_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_dict[:n]\n",
    "\n",
    "def get_negative_words(vectorizer, model, n = 10):\n",
    "    coefs_dict = dict(zip(vectorizer.get_feature_names(), model.coef_[0]))\n",
    "    sorted_dict = sorted(coefs_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_dict[-n:]\n",
    "\n",
    "def train_and_predict_bow(estimator, count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='text'):\n",
    "    print(\"Model info: \" + estimator.__class__.__name__ + \" on dataset \" + column + \" preprocessed with \" \n",
    "          + count_vectorizer.__class__.__name__)\n",
    "    \n",
    "    X_train = df[column]\n",
    "    y_train = df['target']\n",
    "    y_train = y_train.astype('bool')\n",
    "    X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "    estimator.fit(X_train_bow, y_train)  \n",
    "    \n",
    "    X_test_bow = count_vectorizer.transform(test_df[column])\n",
    "    y_test = test_df.target\n",
    "    y_test=y_test.astype('bool')\n",
    "\n",
    "\n",
    "    print(\"accuracy: \" + str(accuracy_score(y_test, estimator.predict(X_test_bow))))\n",
    "    print(\"roc auc: \" + str(roc_auc_score(y_test, estimator.predict(X_test_bow))))\n",
    "    print('positive: ')\n",
    "    print(get_top_words(count_vectorizer, estimator))\n",
    "    print('negative: ')\n",
    "    print(get_negative_words(count_vectorizer, estimator))\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём со стохастического градиентного спуска. Попробуем модель на:\n",
    "- простом тексте без стоп слов и знаков препинаний;\n",
    "- на нём же после стемминга;\n",
    "- на нем же после лемматизации;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.83924\n",
      "roc auc: 0.83924\n",
      "positive: \n",
      "[('refreshing', 4.127016881086339), ('subtle', 3.7301883348280396), ('vengeance', 3.4920912070730625), ('flawless', 3.412725497821397), ('enjoyable', 3.412725497821396), ('rare', 3.1746283700664324), ('perfectly', 3.1746283700664293), ('spectacular', 3.095262660814763), ('wonderfully', 3.0952626608147593), ('complain', 3.095262660814756)]\n",
      "negative: \n",
      "[('horrible', -3.7301883348280436), ('disappointing', -3.888919753331373), ('mstk', -3.968285462583027), ('awful', -3.9682854625830304), ('forgettable', -4.365114008841333), ('lacks', -4.365114008841333), ('poorly', -4.761942555099629), ('waste', -5.1587711013579405), ('worst', -5.317502519861252), ('disappointment', -5.55559964761624)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.83804\n",
      "roc auc: 0.83804\n",
      "positive: \n",
      "[('refresh', 5.23813681060958), ('flawless', 4.047651171834688), ('stallon', 3.9682854625830366), ('excel', 3.7301883348280374), ('perfectli', 3.6508226255763825), ('swim', 3.571456916324735), ('driven', 3.571456916324723), ('highli', 3.4920912070730714), ('mildr', 3.492091207073068), ('spectacular', 3.4920912070730665)]\n",
      "negative: \n",
      "[('badli', -3.8889197533313724), ('forgett', -4.127016881086344), ('mstk', -4.127016881086348), ('dull', -4.127016881086349), ('wors', -4.285748299589673), ('lousi', -4.365114008841326), ('worst', -4.603211136596308), ('wast', -4.841308264351298), ('aw', -4.9206739736029546), ('poorli', -5.793696775371216)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.83712\n",
      "roc auc: 0.8371200000000001\n",
      "positive: \n",
      "[('refresh', 4.285748299589671), ('perfectly', 4.127016881086347), ('superb', 3.8889197533313737), ('wonderfully', 3.888919753331364), ('enjoyable', 3.7301883348280533), ('highly', 3.7301883348280502), ('surprisingly', 3.7301883348280436), ('delightful', 3.650822625576381), ('flawless', 3.571456916324721), ('favorite', 3.4920912070730687)]\n",
      "negative: \n",
      "[('miscast', -3.7301883348280507), ('unfunny', -4.047651171834684), ('dreadful', -4.127016881086346), ('mess', -4.127016881086349), ('lousy', -4.12701688108635), ('waste', -4.603211136596287), ('disappointment', -4.761942555099626), ('awful', -4.841308264351297), ('worst', -5.1587711013579405), ('poorly', -5.2381368106096)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text')\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text')\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как ни странно, стемминг дал чуть лучший результат чем лемматизация, последняя в свою очередь лучше простого текста. Правда, тут видна некоторая утечка данных - такие слова как '710', '810', '1010'  есть ни что иное, как оценки пользователей без символа '/'. В принципе их вполне можно считать как признаки, но для вычисления именно значящих слов, после первого теста моделей создадим ещё один вариант входных значений без цифр и чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим n-gram = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.84372\n",
      "roc auc: 0.8437199999999999\n",
      "positive: \n",
      "[('refreshing', 4.444479718092982), ('well worth', 4.047651171834684), ('wonderfully', 3.8889197533313684), ('funniest', 3.888919753331367), ('must see', 3.5714569163247245), ('rare', 3.571456916324721), ('today', 3.492091207073062), ('superb', 3.412725497821396), ('flawless', 3.333359788569741), ('vengeance', 3.1746283700664204)]\n",
      "negative: \n",
      "[('horrible', -3.809554044079705), ('lousy', -3.8095540440797064), ('weak', -3.809554044079711), ('forgettable', -4.206382590338005), ('baldwin', -4.365114008841326), ('worst', -4.523845427344644), ('awful', -4.603211136596322), ('waste', -4.920673973602949), ('poorly', -5.000039682854611), ('disappointment', -5.7936967753712025)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84248\n",
      "roc auc: 0.8424800000000001\n",
      "positive: \n",
      "[('well worth', 4.206382590338015), ('funniest', 3.888919753331363), ('must see', 3.8095540440797127), ('subtl', 3.7301883348280422), ('driven', 3.6508226255763834), ('excel', 3.6508226255763834), ('refresh', 3.6508226255763825), ('surprisingli', 3.492091207073064), ('perfectli', 3.4920912070730625), ('mildr', 3.4920912070730474)]\n",
      "negative: \n",
      "[('insult', -3.8095540440796967), ('laughabl', -3.809554044079702), ('pointless', -3.809554044079702), ('lousi', -3.968285462583024), ('mstk', -3.968285462583028), ('unfunni', -3.968285462583028), ('baldwin', -4.127016881086347), ('wors', -4.52384542734465), ('aw', -5.238136810609591), ('poorli', -5.873062484622879)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84692\n",
      "roc auc: 0.8469200000000001\n",
      "positive: \n",
      "[('wonderfully', 4.047651171834684), ('vengeance', 4.0476511718346835), ('refresh', 3.9682854625830206), ('well worth', 3.9682854625830184), ('funniest', 3.888919753331364), ('mildred', 3.809554044079709), ('simple', 3.8095540440797047), ('excellent', 3.8095540440797033), ('highly recommend', 3.73018833482804), ('easy', 3.5714569163247276)]\n",
      "negative: \n",
      "[('lousy', -3.809554044079701), ('worst', -3.809554044079711), ('mstk', -3.968285462583029), ('mess', -4.047651171834692), ('unfunny', -4.1270168810863455), ('awful', -4.206382590338003), ('waste', -4.285748299589671), ('laughable', -4.6825768458479695), ('disappointment', -5.39686822911292), ('poorly', -6.111159612377858)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На биграммах лучше показал себя простой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.8326\n",
      "roc auc: 0.8326\n",
      "positive: \n",
      "[('vengeance', 3.888919753331351), ('wonderfully', 3.8095540440797087), ('refreshing', 3.7301883348280453), ('well worth', 3.7301883348280405), ('funniest', 3.571456916324718), ('surprisingly good', 3.4920912070730608), ('mildred', 3.2539940793180846), ('surprisingly', 3.2539940793180793), ('alexandre', 3.2539940793180744), ('incredible', 3.253994079318073)]\n",
      "negative: \n",
      "[('worst', -3.6508226255763927), ('worse', -3.7301883348280453), ('lacks', -3.7301883348280485), ('laughable', -3.8095540440797055), ('awful', -3.8095540440797073), ('horrible', -3.8889197533313555), ('lousy', -3.968285462583027), ('waste', -4.365114008841337), ('poorly', -4.920673973602952), ('disappointment', -5.952428193874534)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.80624\n",
      "roc auc: 0.8062399999999998\n",
      "positive: \n",
      "[('favorit', 4.44447971809299), ('refresh', 4.206382590338008), ('excel', 4.206382590337996), ('funniest', 3.8095540440797038), ('mildr', 3.65082262557639), ('must see', 3.6508226255763865), ('well worth', 3.571456916324723), ('vengeanc', 3.5714569163247205), ('delight', 3.5714569163247196), ('perfectli', 3.4920912070730745)]\n",
      "negative: \n",
      "[('pointless', -3.9682854625830206), ('forgett', -4.047651171834681), ('lousi', -4.127016881086352), ('mediocr', -4.127016881086352), ('embarrass', -4.365114008841326), ('wors', -4.444479718092973), ('unfunni', -4.444479718092985), ('mstk', -4.60321113659631), ('aw', -5.000039682854611), ('poorli', -5.71433106611956)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84192\n",
      "roc auc: 0.8419199999999999\n",
      "positive: \n",
      "[('rare', 4.3651140088413305), ('refresh', 4.206382590338013), ('wonderfully', 4.206382590338006), ('funniest', 3.8889197533313715), ('subtle', 3.8889197533313573), ('surprisingly', 3.8095540440797153), ('perfectly', 3.7301883348280556), ('br must', 3.730188334828044), ('criticism', 3.7301883348280396), ('enjoyable', 3.6508226255763803)]\n",
      "negative: \n",
      "[('horrible', -3.968285462583029), ('dull', -3.968285462583039), ('unfunny', -4.047651171834687), ('mstk', -4.206382590338003), ('worst', -4.20638259033801), ('awful', -4.365114008841324), ('waste', -4.365114008841329), ('laughable', -4.523845427344646), ('disappointment', -4.6825768458479615), ('poorly', -5.31750251986125)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Триграммы не принесли новых значимых фич в топ-20, оставим для следующих моделей максимум в 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем байесовский классификатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: MultinomialNB on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.8364\n",
      "roc auc: 0.8364\n",
      "positive: \n",
      "[('br', -3.8972706078900003), ('film', -4.248481392850371), ('movie', -4.325564896712059), ('one', -4.65356390157368), ('like', -5.056850983945498), ('good', -5.222750243966832), ('story', -5.354159763146175), ('great', -5.384204269880783), ('time', -5.431034481509069), ('see', -5.458202270688039)]\n",
      "negative: \n",
      "[('delia', -14.13171521635923), ('hobgoblins', -14.13171521635923), ('kareena', -14.13171521635923), ('kornbluth', -14.13171521635923), ('saif', -14.13171521635923), ('sarne', -14.13171521635923), ('slater', -14.13171521635923), ('tashan', -14.13171521635923), ('tremors', -14.13171521635923), ('welch', -14.13171521635923)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.82156\n",
      "roc auc: 0.8215600000000001\n",
      "positive: \n",
      "[('br', -3.9593016801438665), ('film', -4.091202980291849), ('movi', -4.204081039835916), ('one', -4.668084358381716), ('like', -4.969503011467923), ('time', -5.220268140092061), ('good', -5.270021890906698), ('see', -5.2997618496733025), ('stori', -5.311632244404276), ('charact', -5.358681353582021)]\n",
      "negative: \n",
      "[('savini', -14.193746288613097), ('shaq', -14.193746288613097), ('slater', -14.193746288613097), ('sooki', -14.193746288613097), ('stirba', -14.193746288613097), ('tashan', -14.193746288613097), ('trancer', -14.193746288613097), ('unisol', -14.193746288613097), ('weisz', -14.193746288613097), ('zenia', -14.193746288613097)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.82676\n",
      "roc auc: 0.82676\n",
      "positive: \n",
      "[('br', -3.9419366788025574), ('film', -4.0738379789505395), ('movie', -4.1866242995834835), ('one', -4.650646404277289), ('see', -4.903629655174209), ('make', -4.934445353705854), ('like', -4.972461483098527), ('get', -5.16461381267561), ('well', -5.191938254806436), ('time', -5.204932823577954)]\n",
      "negative: \n",
      "[('palermo', -14.176381287271788), ('rosanna', -14.176381287271788), ('saif', -14.176381287271788), ('sarne', -14.176381287271788), ('savini', -14.176381287271788), ('shaq', -14.176381287271788), ('slater', -14.176381287271788), ('tashan', -14.176381287271788), ('weisz', -14.176381287271788), ('zenia', -14.176381287271788)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='lemm_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: MultinomialNB on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.84704\n",
      "roc auc: 0.84704\n",
      "positive: \n",
      "[('br', -3.9573191818423794), ('film', -4.30852996680275), ('movie', -4.385613470664438), ('one', -4.713612475526059), ('like', -5.116899557897877), ('good', -5.282798817919211), ('story', -5.414208337098554), ('great', -5.444252843833162), ('time', -5.4910830554614485), ('see', -5.518250844640418)]\n",
      "negative: \n",
      "[('thunderbirds', -13.498616609751664), ('total waste', -13.498616609751664), ('uwe', -13.498616609751664), ('uwe boll', -13.498616609751664), ('wayans', -13.498616609751664), ('worst films', -13.498616609751664), ('acting horrible', -14.19176379031161), ('hours life', -14.19176379031161), ('prom night', -14.19176379031161), ('worst acting', -14.19176379031161)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84172\n",
      "roc auc: 0.8417200000000001\n",
      "positive: \n",
      "[('br', -4.044338223920716), ('film', -4.1762395240686985), ('movi', -4.289117583612766), ('one', -4.753120902158566), ('like', -5.054539555244773), ('time', -5.305304683868911), ('good', -5.3550584346835475), ('see', -5.384798393450152), ('stori', -5.396668788181126), ('charact', -5.443717897358871)]\n",
      "negative: \n",
      "[('uwe', -13.585635651830001), ('uwe boll', -13.585635651830001), ('watch paint', -13.585635651830001), ('wayan', -13.585635651830001), ('act horribl', -14.278782832389947), ('prom night', -14.278782832389947), ('slater', -14.278782832389947), ('tashan', -14.278782832389947), ('wast film', -14.278782832389947), ('worst act', -14.278782832389947)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84412\n",
      "roc auc: 0.84412\n",
      "positive: \n",
      "[('br', -4.029666533959464), ('film', -4.161567834107446), ('movie', -4.27435415474039), ('one', -4.738376259434196), ('see', -4.991359510331115), ('make', -5.022175208862761), ('like', -5.060191338255434), ('get', -5.252343667832516), ('well', -5.2796681099633425), ('time', -5.29266267873486)]\n",
      "negative: \n",
      "[('total waste', -13.570963961868749), ('uwe', -13.570963961868749), ('uwe boll', -13.570963961868749), ('wayans', -13.570963961868749), ('act horrible', -14.264111142428694), ('prom night', -14.264111142428694), ('slater', -14.264111142428694), ('tashan', -14.264111142428694), ('waste film', -14.264111142428694), ('worst act', -14.264111142428694)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='lemm_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы видим довольно много распространенных слов, но это происходит во многом из-за их высокой априорной вероятности. В негативных значения поинтереснее(особенно uwe и uwe boll, а также wayans - плохие режиссеры становятся именем нарицательным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: LinearSVC on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.83348\n",
      "roc auc: 0.83348\n",
      "positive: \n",
      "[('flawless', 1.1552537763145996), ('well worth', 1.1392147209892094), ('br must', 1.1192352433061774), ('refreshing', 1.0831790902172522), ('surprisingly good', 1.0509433682998752), ('pickup', 1.0383345022552855), ('hoot', 1.0250721088028092), ('even better', 1.007444100540922), ('marvelous', 0.9829519273581528), ('able see', 0.9736849264754289)]\n",
      "negative: \n",
      "[('lousy', -0.9931604127891982), ('one worst', -0.9951447902897348), ('lifeless', -0.997737390387638), ('dreadful', -1.043698711823755), ('hours life', -1.0588304139935838), ('poorly', -1.0924852184736424), ('forgettable', -1.1311505132453612), ('mstk', -1.148915233089379), ('disappointment', -1.1737277182011188), ('baldwin', -1.238212324376126)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.82736\n",
      "roc auc: 0.82736\n",
      "positive: \n",
      "[('never bore', 1.2544241758736523), ('driven', 1.2025687816713926), ('make bad', 1.1670632672460757), ('firefight', 1.1145682680763227), ('br anim', 1.089720137014273), ('surprisingli good', 1.0643324032485957), ('kurosawa', 1.044037724834478), ('rather good', 1.0301811516570514), ('well worth', 1.0298924193273475), ('refresh', 1.0204244888045224)]\n",
      "negative: \n",
      "[('movi goe', -1.0047123614899802), ('wouldnt recommend', -1.046269182264177), ('unfunni', -1.0684395200354633), ('lousi', -1.0686824372487331), ('unwatch', -1.0759550135663014), ('brainwash', -1.1071611203701985), ('tunnel', -1.1142968356490623), ('poorli', -1.2086054804700421), ('mstk', -1.3628575315908946), ('baldwin', -1.4784450882569031)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.82744\n",
      "roc auc: 0.8274400000000001\n",
      "positive: \n",
      "[('well worth', 1.2036900658831768), ('refresh', 1.1724860773826), ('subsequently', 1.1710390479725217), ('kurosawa', 1.1393609619344074), ('br must', 1.031575721750679), ('vengeance', 1.0297377478770788), ('definitive', 1.0265855449742574), ('whoopi', 1.0229637767441122), ('erotic', 1.0160493547872473), ('flawless', 0.99827784023101)]\n",
      "negative: \n",
      "[('wouldnt recommend', -1.0460174066584875), ('miscast', -1.0669158357367934), ('sever', -1.086780178975202), ('two great', -1.1039934744852289), ('unfunny', -1.1371092434682424), ('poorly', -1.1425291534675244), ('disappointment', -1.1508932089465085), ('skip one', -1.2071048562982063), ('lifeless', -1.2368985291882129), ('baldwin', -1.322953998378042)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим все те же операции, но теперь с TF-IDF вместо Bag-of-Words. Будем использовать сразу биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88528\n",
      "roc auc: 0.8852800000000001\n",
      "positive: \n",
      "[('excellent', 3.6851802375723444), ('great', 3.622783375328195), ('perfect', 2.8668162303378333), ('best', 2.7884322364604253), ('wonderful', 2.711613125191703), ('amazing', 2.617981408422977), ('favorite', 2.5358047435466973), ('fun', 2.4392112748273194), ('today', 2.180208917040142), ('brilliant', 2.1556243691031556)]\n",
      "negative: \n",
      "[('unfortunately', -2.83793542494793), ('disappointment', -2.898411813104424), ('terrible', -2.9223978653043434), ('nothing', -3.065593966991999), ('waste', -3.453917075793234), ('poor', -3.476058421171242), ('boring', -3.4968406384410264), ('awful', -3.935910204335568), ('bad', -4.010650715468367), ('worst', -4.9618553995989565)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88292\n",
      "roc auc: 0.8829199999999999\n",
      "positive: \n",
      "[('excel', 3.951958068207169), ('great', 3.7921424196672313), ('enjoy', 2.9955400349502215), ('perfect', 2.960619432603787), ('favorit', 2.585352643872794), ('best', 2.576344262131631), ('love', 2.5003846906402463), ('fun', 2.4038673115606937), ('today', 2.2866181433730053), ('amaz', 2.2661213160736557)]\n",
      "negative: \n",
      "[('terribl', -2.888534761416477), ('fail', -2.997036256735478), ('noth', -3.1518610915057077), ('disappoint', -3.3192758658177026), ('poor', -3.434060868179545), ('bore', -3.8272005552743953), ('wast', -3.9759706277855242), ('bad', -4.077442784719946), ('aw', -4.266680958445929), ('worst', -5.006878063580237)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88452\n",
      "roc auc: 0.88452\n",
      "positive: \n",
      "[('great', 3.6925708681502036), ('excellent', 3.689890262265153), ('perfect', 2.8576643532232024), ('wonderful', 2.7100719280389844), ('best', 2.6234540106859496), ('love', 2.4571301996862176), ('favorite', 2.439181511934154), ('fun', 2.4389745741078523), ('enjoy', 2.422105565862605), ('amaze', 2.3713346405720808)]\n",
      "negative: \n",
      "[('unfortunately', -2.8707013699869917), ('fail', -2.9798882481016316), ('nothing', -3.045493989531999), ('dull', -3.0983005536687336), ('poor', -3.4382636780461233), ('boring', -3.4620957324004706), ('waste', -4.064681000267777), ('awful', -4.151751967862), ('bad', -4.746116464239355), ('worst', -5.0303827293128665)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.85348\n",
      "roc auc: 0.85348\n",
      "positive: \n",
      "[('br', -5.154928261645315), ('film', -5.377246165380087), ('movie', -5.382474910706257), ('one', -5.805642217959858), ('great', -6.018149924496492), ('good', -6.074607044220919), ('like', -6.077736087309303), ('story', -6.140905883832041), ('see', -6.234810919639754), ('well', -6.2659180829261825)]\n",
      "negative: \n",
      "[('even worth', -11.547090977845018), ('skip one', -11.547955146120744), ('total waste', -11.55564658124817), ('br worst', -11.573041302309042), ('possibly worst', -11.576377118023645), ('thunderbirds', -11.577776242847817), ('acting horrible', -11.637079547364745), ('hours life', -11.637079547364745), ('prom night', -11.637079547364745), ('worst acting', -11.637079547364745)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.84908\n",
      "roc auc: 0.84908\n",
      "positive: \n",
      "[('br', -5.197334762198611), ('film', -5.262321872110003), ('movi', -5.28947891727319), ('one', -5.815145250321715), ('like', -5.999854473461357), ('great', -6.048979522611759), ('good', -6.1010234135629045), ('stori', -6.105940436228879), ('love', -6.117240481800284), ('time', -6.137596726271707)]\n",
      "negative: \n",
      "[('br worst', -11.587593766787466), ('thunderbird', -11.58923383873991), ('ajay', -11.589616854619058), ('possibl worst', -11.589890889690768), ('act horribl', -11.647193046389004), ('prom night', -11.647193046389004), ('slater', -11.647193046389004), ('tashan', -11.647193046389004), ('wast film', -11.647193046389004), ('worst act', -11.647193046389004)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.8518\n",
      "roc auc: 0.8518\n",
      "positive: \n",
      "[('br', -5.182037325828199), ('film', -5.247527932000266), ('movie', -5.278020215226831), ('one', -5.801215316004873), ('see', -5.885973783682112), ('like', -5.9973953647208935), ('make', -6.020876899913221), ('great', -6.035179228161451), ('story', -6.09360255662221), ('good', -6.094746647416973)]\n",
      "negative: \n",
      "[('br worst', -11.581986873981503), ('possibly worst', -11.584805085194535), ('ajay', -11.584975778829264), ('thunderbird', -11.58639787761716), ('act horrible', -11.644305455688821), ('prom night', -11.644305455688821), ('slater', -11.644305455688821), ('tashan', -11.644305455688821), ('waste film', -11.644305455688821), ('worst act', -11.644305455688821)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.8696\n",
      "roc auc: 0.8695999999999999\n",
      "positive: \n",
      "[('excellent', 3.1391181691431225), ('perfect', 2.806862907278241), ('refreshing', 2.5981794963556673), ('well worth', 2.5851228532547776), ('funniest', 2.489583515382711), ('wonderful', 2.412258710663851), ('enjoyable', 2.394932655270455), ('great', 2.3904205782233467), ('flawless', 2.325259176988842), ('wonderfully', 2.27094662579809)]\n",
      "negative: \n",
      "[('disappointing', -2.8053878953881144), ('horrible', -2.8207018735944964), ('boring', -2.8437766405880236), ('weak', -2.8713660912422783), ('lacks', -2.92176188200717), ('awful', -3.5029801777990173), ('poorly', -3.55169891825744), ('waste', -3.615969427601141), ('disappointment', -3.7382146624536605), ('worst', -4.340458842217668)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.869\n",
      "roc auc: 0.8689999999999999\n",
      "positive: \n",
      "[('excel', 3.4192685088609522), ('perfect', 2.8273474703690886), ('well worth', 2.636531611865037), ('refresh', 2.4941367862139763), ('great', 2.4787537192991445), ('favorit', 2.453448772648724), ('must see', 2.396438433262726), ('funniest', 2.293150955196657), ('perfectli', 2.247660066147091), ('never bore', 2.1956806487885334)]\n",
      "negative: \n",
      "[('dull', -2.6438625874076176), ('poor', -2.64915017589769), ('mstk', -2.658710916923033), ('wors', -2.9431999805713134), ('disappoint', -3.076063699412136), ('wast', -3.114705935787758), ('bore', -3.3014256283985652), ('poorli', -3.6274868526354087), ('aw', -4.008063812310102), ('worst', -4.332946132933541)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.86836\n",
      "roc auc: 0.86836\n",
      "positive: \n",
      "[('excellent', 3.2119988143459475), ('perfect', 2.838867174838067), ('well worth', 2.7012462445509358), ('refresh', 2.4963850451788154), ('great', 2.491970304989897), ('funniest', 2.4882559014619976), ('favorite', 2.4002542304456553), ('wonderful', 2.369460775702109), ('flawless', 2.3167338976279863), ('rare', 2.3062430150734587)]\n",
      "negative: \n",
      "[('poor', -2.7277821440469636), ('dull', -2.8024052478393866), ('horrible', -2.889802259678549), ('bad', -2.981850408155018), ('boring', -3.019548537835785), ('waste', -3.1734647172387995), ('disappointment', -3.41862447069323), ('poorly', -3.5399384712821633), ('awful', -3.796848886880824), ('worst', -4.211058098559953)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='lemm_text')\n",
    "\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='stem_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='lemm_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
