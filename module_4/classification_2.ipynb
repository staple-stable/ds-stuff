{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aleksandr_Mishin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных в датафрейм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_DATA = './test/'\n",
    "PATH_TO_TRAIN_DATA = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_set(path_to_files):\n",
    "    df = pd.DataFrame(columns=['text', 'target'])\n",
    "    positive_files = glob(path_to_files + \"pos/*.txt\")\n",
    "    negative_files = glob(path_to_files + \"neg/*.txt\")\n",
    "    \n",
    "    for file in tqdm(positive_files):\n",
    "        with open(file, 'r', encoding=\"utf8\") as content_file:\n",
    "            content = content_file.read()\n",
    "            df.loc[df.shape[0]] = [content, 1]\n",
    "    for file in tqdm(negative_files):\n",
    "        with open(file, 'r', encoding=\"utf8\") as content_file:\n",
    "            content = content_file.read()\n",
    "            df.loc[df.shape[0]] = [content, 0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552a9691f2e044cfb7a0f2bf23e62f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363d65af85cc4deca9ec918da30d9ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = prepare_train_set(PATH_TO_TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сериализуем датафрейм, чтобы не пересчитывать при перезапуске ядра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./pickle/df.pkl\", 'wb') as df_pkl:\n",
    "    pickle.dump(df, df_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"./pickle/df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.&lt;br /&gt;&lt;br /&gt;But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.&lt;br /&gt;&lt;br /&gt;Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.&lt;br /&gt;&lt;br /&gt;While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.&lt;br /&gt;&lt;br /&gt;Or maybe this film will inspire you to help others.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "0  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1  Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.   \n",
       "2  Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "  target  \n",
       "0  1      \n",
       "1  1      \n",
       "2  1      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df.text = df.text.apply(lambda x: x.lower())\n",
    "\n",
    "tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "df.text = df.text.apply(lambda x: x.translate(tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt   \n",
       "\n",
       "  target  \n",
       "0  1      "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stoplist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homelessness houselessness george carlin stated issue years never plan help street considered human everything going school work vote matter people think homeless lost cause worrying things racism war iraq pressuring kids succeed technology elections inflation worrying theyll next end streetsbr br given bet live streets month without luxuries home entertainment sets bathroom pictures wall computer everything treasure see like homeless goddard bolts lessonbr br mel brooks directs stars bolt plays rich man everything world deciding make bet sissy rival jeffery tambor see live streets thirty days without luxuries bolt succeeds wants future project making buildings bets bolt thrown street bracelet leg monitor every move cant step sidewalk hes given nickname pepto vagrant written forehead bolt meets characters including woman name molly lesley ann warren exdancer got divorce losing home pals sailor howard morris fumes teddy wilson already used streets theyre survivors bolt isnt hes used reaching mutual agreements like rich fight flight kill killedbr br love connection molly bolt wasnt necessary plot found life stinks one mel brooks observant films prior comedy shows tender side compared slapstick work blazing saddles young frankenstein spaceballs matter show like something valuable losing next day hand making stupid bet like rich people dont know money maybe give homeless instead using like monopoly moneybr br maybe film inspire help others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brilliant overacting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse second none corn face classic good anything blazing saddles take lawyers also superb accused turncoat selling boss dishonest lawyer pepto bolt shrugs indifferently im lawyer says three funny words jeffrey tambor favorite later larry sanders show fantastic mad millionaire wants crush ghetto character malevolent usual hospital scene scene homeless invade demolition site alltime classics look legs scene two big diggers fighting one bleeds movie gets better time see quite often</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "1  homelessness houselessness george carlin stated issue years never plan help street considered human everything going school work vote matter people think homeless lost cause worrying things racism war iraq pressuring kids succeed technology elections inflation worrying theyll next end streetsbr br given bet live streets month without luxuries home entertainment sets bathroom pictures wall computer everything treasure see like homeless goddard bolts lessonbr br mel brooks directs stars bolt plays rich man everything world deciding make bet sissy rival jeffery tambor see live streets thirty days without luxuries bolt succeeds wants future project making buildings bets bolt thrown street bracelet leg monitor every move cant step sidewalk hes given nickname pepto vagrant written forehead bolt meets characters including woman name molly lesley ann warren exdancer got divorce losing home pals sailor howard morris fumes teddy wilson already used streets theyre survivors bolt isnt hes used reaching mutual agreements like rich fight flight kill killedbr br love connection molly bolt wasnt necessary plot found life stinks one mel brooks observant films prior comedy shows tender side compared slapstick work blazing saddles young frankenstein spaceballs matter show like something valuable losing next day hand making stupid bet like rich people dont know money maybe give homeless instead using like monopoly moneybr br maybe film inspire help others   \n",
       "2  brilliant overacting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse second none corn face classic good anything blazing saddles take lawyers also superb accused turncoat selling boss dishonest lawyer pepto bolt shrugs indifferently im lawyer says three funny words jeffrey tambor favorite later larry sanders show fantastic mad millionaire wants crush ghetto character malevolent usual hospital scene scene homeless invade demolition site alltime classics look legs scene two big diggers fighting one bleeds movie gets better time see quite often                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "  target  \n",
       "0  1      \n",
       "1  1      \n",
       "2  1      "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим PorterStemer на входные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "df['stem_text'] = df['text'].apply(stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также применим лемматизацию в виде отдельного признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = sentence.split()\n",
    "    lemmed_tokens = [lemmatizer.lemmatize(token, get_pos(token)) for token in tokens]\n",
    "    return ' '.join(lemmed_tokens)\n",
    "\n",
    "df['lemm_text'] = df['text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwel high cartoon comedi ran time program school life teacher 35 year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt</td>\n",
       "      <td>bromwell high cartoon comedy run time program school life teacher 35 year teaching profession lead believe bromwell high satire much close reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school know student saw episode student repeatedly try burn school immediately recall high classic line inspector im sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetch pity isnt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt   \n",
       "\n",
       "  target  \\\n",
       "0  1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                  stem_text  \\\n",
       "0  bromwel high cartoon comedi ran time program school life teacher 35 year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       lemm_text  \n",
       "0  bromwell high cartoon comedy run time program school life teacher 35 year teaching profession lead believe bromwell high satire much close reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school know student saw episode student repeatedly try burn school immediately recall high classic line inspector im sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetch pity isnt  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим аналогичные операции для отложенной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1928713abcb8433cb325948012575a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154202b940da4a4f9f8acd7bd940be85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = prepare_train_set(PATH_TO_TEST_DATA)\n",
    "test_df.text = test_df.text.apply(lambda x: x.lower())\n",
    "test_df.text = test_df.text.apply(lambda x: x.translate(tr))\n",
    "test_df.text = test_df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stoplist]))\n",
    "test_df['stem_text'] = test_df['text'].apply(stem_sentence)\n",
    "test_df['lemm_text'] = test_df['text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим bag-of-words для получения фич по токенам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(vectorizer, model, n = 10):\n",
    "    coefs_dict = dict(zip(vectorizer.get_feature_names(), model.coef_[0]))\n",
    "    sorted_dict = sorted(coefs_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_dict[:n]\n",
    "\n",
    "def get_negative_words(vectorizer, model, n = 10):\n",
    "    coefs_dict = dict(zip(vectorizer.get_feature_names(), model.coef_[0]))\n",
    "    sorted_dict = sorted(coefs_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_dict[-n:]\n",
    "\n",
    "def train_and_predict_bow(estimator, count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='text'):\n",
    "    print(\"Model info: \" + estimator.__class__.__name__ + \" on dataset \" + column + \" preprocessed with \" \n",
    "          + count_vectorizer.__class__.__name__)\n",
    "    \n",
    "    X_train = df[column]\n",
    "    y_train = df['target']\n",
    "    y_train = y_train.astype('bool')\n",
    "    X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "    estimator.fit(X_train_bow, y_train)  \n",
    "    \n",
    "    X_test_bow = count_vectorizer.transform(test_df[column])\n",
    "    y_test = test_df.target\n",
    "    y_test=y_test.astype('bool')\n",
    "\n",
    "\n",
    "    print(\"accuracy: \" + str(accuracy_score(y_test, estimator.predict(X_test_bow))))\n",
    "    print(\"roc auc: \" + str(roc_auc_score(y_test, estimator.predict(X_test_bow))))\n",
    "    print('positive: ')\n",
    "    print(get_top_words(count_vectorizer, estimator))\n",
    "    print('negative: ')\n",
    "    print(get_negative_words(count_vectorizer, estimator))\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём со стохастического градиентного спуска. Попробуем модель на:\n",
    "- простом тексте без стоп слов и знаков препинаний;\n",
    "- на нём же после стемминга;\n",
    "- на нем же после лемматизации;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.84548\n",
      "roc auc: 0.84548\n",
      "positive: \n",
      "[('710', 7.460376669656075), ('810', 4.523845427344654), ('flawless', 3.888919753331356), ('vengeance', 3.730188334828033), ('wonderfully', 3.6508226255763856), ('refreshing', 3.571456916324724), ('excellent', 3.4920912070730687), ('funniest', 3.492091207073062), ('perfectly', 3.4127254978214006), ('spectacular', 3.412725497821399)]\n",
      "negative: \n",
      "[('lousy', -3.7301883348280476), ('mst3k', -3.8889197533313644), ('awful', -3.9682854625830286), ('310', -4.206382590338005), ('forgettable', -4.523845427344646), ('worst', -4.761942555099597), ('disappointment', -4.9206739736029474), ('poorly', -5.079405392106276), ('waste', -5.39686822911293), ('410', -6.58735386788782)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.8356\n",
      "roc auc: 0.8356\n",
      "positive: \n",
      "[('710', 7.936570925166042), ('810', 5.476233938364572), ('refresh', 4.44447971809299), ('perfectli', 4.285748299589675), ('flawless', 4.285748299589667), ('subtl', 4.047651171834684), ('excel', 3.8889197533313378), ('surprisingli', 3.730188334828044), ('funniest', 3.6508226255763847), ('delight', 3.5714569163247245)]\n",
      "negative: \n",
      "[('unfunni', -4.047651171834681), ('lousi', -4.127016881086346), ('mst3k', -4.206382590338004), ('forgett', -4.44447971809299), ('wast', -4.8413082643512775), ('worst', -5.0000396828546085), ('310', -5.000039682854613), ('aw', -5.238136810609605), ('poorli', -6.507988158636151), ('410', -7.063548123397778)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.80532\n",
      "roc auc: 0.80532\n",
      "positive: \n",
      "[('710', 7.381010960404421), ('810', 5.000039682854608), ('rare', 4.603211136596313), ('wonderfully', 4.127016881086355), ('refresh', 4.047651171834686), ('1010', 4.0476511718346835), ('excellent', 3.6508226255763754), ('highly', 3.5714569163247294), ('criticism', 3.4920912070730656), ('vengeance', 3.4127254978213983)]\n",
      "negative: \n",
      "[('unfortunately', -3.8889197533313706), ('mst3k', -3.9682854625830335), ('pointless', -4.047651171834685), ('waste', -4.365114008841321), ('310', -4.444479718092985), ('worst', -4.761942555099623), ('awful', -4.9206739736029474), ('disappointment', -5.07940539210628), ('poorly', -5.8730624846228725), ('410', -7.857205215914381)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text')\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text')\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как ни странно, стемминг дал чуть лучший результат чем лемматизация, последняя в свою очередь лучше простого текста. Правда, тут видна некоторая утечка данных - такие слова как '710', '810', '1010'  есть ни что иное, как оценки пользователей без символа '/'. В принципе их вполне можно считать как признаки, но для вычисления именно значящих слов, после первого теста моделей создадим ещё один вариант входных значений без цифр и чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим n-gram = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.8492\n",
      "roc auc: 0.8491999999999998\n",
      "positive: \n",
      "[('710', 7.6984737974110695), ('refreshing', 4.047651171834681), ('810', 3.8889197533313644), ('funniest', 3.809554044079708), ('well worth', 3.492091207073062), ('vengeance', 3.412725497821397), ('1010', 3.3333597885697435), ('wonderfully', 3.3333597885697395), ('subtle', 3.2539940793180846), ('ladder', 3.1746283700664213)]\n",
      "negative: \n",
      "[('mst3k', -3.7301883348280382), ('forgettable', -3.888919753331368), ('awful', -3.9682854625830197), ('lacks', -4.206382590338007), ('310', -4.365114008841325), ('worst', -4.523845427344635), ('waste', -5.158771101357935), ('disappointment', -5.39686822911292), ('poorly', -5.634965356867892), ('410', -6.587353867887821)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.85132\n",
      "roc auc: 0.8513200000000001\n",
      "positive: \n",
      "[('710', 7.381010960404424), ('810', 5.634965356867894), ('well worth', 4.047651171834684), ('refresh', 3.9682854625830273), ('excel', 3.8889197533313964), ('vengeanc', 3.8095540440797038), ('mildr', 3.6508226255763927), ('must see', 3.6508226255763883), ('stallon', 3.5714569163247174), ('subtl', 3.4920912070730705)]\n",
      "negative: \n",
      "[('bell', -3.968285462583027), ('mess', -4.047651171834691), ('lousi', -4.127016881086341), ('mst3k', -4.206382590337999), ('unfunni', -4.285748299589662), ('worst', -4.3651140088413145), ('aw', -4.444479718092965), ('poorli', -4.6825768458479695), ('310', -4.841308264351289), ('410', -7.619108088159393)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.83304\n",
      "roc auc: 0.83304\n",
      "positive: \n",
      "[('710', 7.93657092516605), ('810', 4.603211136596306), ('1010', 4.365114008841331), ('wonderfully', 4.12701688108635), ('rare', 4.127016881086347), ('well worth', 4.047651171834683), ('funniest', 3.888919753331377), ('refresh', 3.888919753331363), ('perfectly', 3.8095540440797118), ('mildred', 3.8095540440796993)]\n",
      "negative: \n",
      "[('forgettable', -4.047651171834688), ('embarrass', -4.047651171834693), ('horrible', -4.047651171834693), ('laughable', -4.2063825903380065), ('worst', -4.6032111365963155), ('awful', -4.92067397360296), ('310', -5.000039682854608), ('disappointment', -5.317502519861252), ('poorly', -5.952428193874528), ('410', -7.301645251152767)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На биграммах лучше показал себя простой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.85104\n",
      "roc auc: 0.8510399999999999\n",
      "positive: \n",
      "[('710', 7.3016452511527685), ('funniest', 4.047651171834699), ('well worth', 3.650822625576386), ('edie', 3.571456916324717), ('810', 3.4920912070730603), ('subtle', 3.412725497821411), ('rare', 3.4127254978214063), ('incredible', 3.4127254978213997), ('vengeance', 3.4127254978213983), ('refreshing', 3.3333597885697412)]\n",
      "negative: \n",
      "[('baldwin', -3.650822625576381), ('mildly', -3.730188334828044), ('worst', -3.8095540440797055), ('310', -4.047651171834691), ('forgettable', -4.047651171834692), ('awful', -4.365114008841313), ('waste', -4.841308264351293), ('disappointment', -5.000039682854614), ('poorly', -5.634965356867895), ('410', -5.714331066119558)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.85088\n",
      "roc auc: 0.8508800000000001\n",
      "positive: \n",
      "[('710', 7.777839506662717), ('810', 5.238136810609593), ('surprisingli', 4.206382590338019), ('must see', 4.206382590337999), ('refresh', 3.968285462583028), ('kitti', 3.8889197533313578), ('1010', 3.730188334828051), ('well worth', 3.730188334828042), ('vengeanc', 3.7301883348280414), ('mildr', 3.6508226255763816)]\n",
      "negative: \n",
      "[('lousi', -3.9682854625830224), ('210', -3.968285462583025), ('mess', -3.968285462583028), ('worst', -4.047651171834701), ('forgett', -4.127016881086357), ('embarrass', -4.206382590338008), ('310', -4.365114008841327), ('aw', -5.000039682854599), ('poorli', -5.5555996476162335), ('410', -6.984182414146125)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84696\n",
      "roc auc: 0.8469600000000002\n",
      "positive: \n",
      "[('710', 6.984182414146121), ('wonderfully', 4.365114008841329), ('810', 4.365114008841327), ('funniest', 3.9682854625830264), ('refresh', 3.9682854625830255), ('1010', 3.9682854625830224), ('subtle', 3.8889197533313733), ('well worth', 3.809554044079707), ('incredible', 3.730188334828045), ('superb', 3.6508226255763834)]\n",
      "negative: \n",
      "[('unfunny', -3.968285462583032), ('forgettable', -4.0476511718346835), ('worst', -4.206382590337999), ('lousy', -4.206382590338007), ('310', -4.603211136596308), ('awful', -4.761942555099635), ('laughable', -5.000039682854607), ('disappointment', -5.000039682854613), ('poorly', -5.873062484622882), ('410', -6.666719577139471)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Триграммы не принесли новых значимых фич в топ-20, оставим для следующих моделей максимум в 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем байесовский классификатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: MultinomialNB on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.83832\n",
      "roc auc: 0.8383200000000001\n",
      "positive: \n",
      "[('br', -3.912733796080138), ('film', -4.252938887208872), ('movie', -4.329979539597176), ('one', -4.657868315181787), ('like', -5.061155397553605), ('good', -5.22718983837947), ('story', -5.358464176754282), ('great', -5.3886675523429854), ('time', -5.436338228977824), ('see', -5.462506684296146)]\n",
      "negative: \n",
      "[('darkman', -14.136019629967338), ('delia', -14.136019629967338), ('hobgoblins', -14.136019629967338), ('kareena', -14.136019629967338), ('kornbluth', -14.136019629967338), ('sarne', -14.136019629967338), ('slater', -14.136019629967338), ('tashan', -14.136019629967338), ('tremors', -14.136019629967338), ('welch', -14.136019629967338)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.8236\n",
      "roc auc: 0.8236\n",
      "positive: \n",
      "[('br', -3.9751872031626903), ('film', -4.0960526619146105), ('movi', -4.208899535600633), ('one', -4.672811106818509), ('like', -4.974229759904716), ('time', -5.225882368777279), ('good', -5.274881839476889), ('see', -5.304488598110096), ('stori', -5.316358992841069), ('charact', -5.3634081020188145)]\n",
      "negative: \n",
      "[('sarn', -14.19847303704989), ('savini', -14.19847303704989), ('shaq', -14.19847303704989), ('slater', -14.19847303704989), ('stirba', -14.19847303704989), ('tashan', -14.19847303704989), ('trancer', -14.19847303704989), ('unisol', -14.19847303704989), ('weisz', -14.19847303704989), ('zenia', -14.19847303704989)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.82948\n",
      "roc auc: 0.8294800000000001\n",
      "positive: \n",
      "[('br', -3.957878437947562), ('film', -4.078743896699482), ('movie', -4.191499023057581), ('one', -4.655429388840263), ('see', -4.9089764965325), ('make', -4.939228338268828), ('like', -4.977244467661501), ('get', -5.1693967972385835), ('well', -5.196846591923633), ('time', -5.210605092325812)]\n",
      "negative: \n",
      "[('palermo', -14.181164271834762), ('rosanna', -14.181164271834762), ('saif', -14.181164271834762), ('sarne', -14.181164271834762), ('savini', -14.181164271834762), ('shaq', -14.181164271834762), ('slater', -14.181164271834762), ('tashan', -14.181164271834762), ('weisz', -14.181164271834762), ('zenia', -14.181164271834762)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,1)), column='lemm_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: MultinomialNB on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.84984\n",
      "roc auc: 0.8498399999999999\n",
      "positive: \n",
      "[('br', -3.9714711056751852), ('film', -4.311676196803919), ('movie', -4.388716849192223), ('one', -4.716605624776834), ('like', -5.119892707148653), ('good', -5.285927147974517), ('story', -5.417201486349329), ('great', -5.447404861938033), ('time', -5.495075538572872), ('see', -5.5212439938911935)]\n",
      "negative: \n",
      "[('total waste', -13.50160975900244), ('uwe', -13.50160975900244), ('uwe boll', -13.50160975900244), ('wayans', -13.50160975900244), ('worst films', -13.50160975900244), ('acting horrible', -14.194756939562385), ('br 410', -14.194756939562385), ('hours life', -14.194756939562385), ('prom night', -14.194756939562385), ('worst acting', -14.194756939562385)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84372\n",
      "roc auc: 0.84372\n",
      "positive: \n",
      "[('br', -4.058544702002372), ('film', -4.179410160754292), ('movi', -4.292257034440315), ('one', -4.756168605658191), ('like', -5.057587258744398), ('time', -5.309239867616961), ('good', -5.3582393383165705), ('see', -5.387846096949778), ('stori', -5.399716491680751), ('charact', -5.446765600858496)]\n",
      "negative: \n",
      "[('uwe', -13.588683355329627), ('uwe boll', -13.588683355329627), ('watch paint', -13.588683355329627), ('wayan', -13.588683355329627), ('act horribl', -14.281830535889572), ('br 410', -14.281830535889572), ('prom night', -14.281830535889572), ('slater', -14.281830535889572), ('wast film', -14.281830535889572), ('worst act', -14.281830535889572)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.84728\n",
      "roc auc: 0.84728\n",
      "positive: \n",
      "[('br', -4.04368498282987), ('film', -4.16455044158179), ('movie', -4.277305567939889), ('one', -4.741235933722571), ('see', -4.994783041414808), ('make', -5.025034883151136), ('like', -5.063051012543809), ('get', -5.2552033421208915), ('well', -5.282653136805941), ('time', -5.29641163720812)]\n",
      "negative: \n",
      "[('total waste', -13.573823636157124), ('uwe', -13.573823636157124), ('uwe boll', -13.573823636157124), ('wayans', -13.573823636157124), ('act horrible', -14.26697081671707), ('br 410', -14.26697081671707), ('prom night', -14.26697081671707), ('slater', -14.26697081671707), ('waste film', -14.26697081671707), ('worst act', -14.26697081671707)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)), column='lemm_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы видим довольно много распространенных слов, но это происходит во многом из-за их высокой априорной вероятности. В негативных значения поинтереснее(особенно uwe и uwe boll, а также wayans - плохие режиссеры становятся именем нарицательным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: LinearSVC on dataset text preprocessed with CountVectorizer\n",
      "accuracy: 0.83704\n",
      "roc auc: 0.83704\n",
      "positive: \n",
      "[('710', 1.8118395485843473), ('refreshing', 1.1422228019450797), ('well worth', 1.0982276008608238), ('even better', 1.0499683521851582), ('able see', 1.0441149755981602), ('surprisingly good', 1.0347886258685592), ('pickup', 1.0265251048854012), ('michael jackson', 0.9992343560880729), ('flawless', 0.9953712049967014), ('respectable', 0.9677207265278389)]\n",
      "negative: \n",
      "[('one worst', -0.9950760137023653), ('poorly', -1.0512802299856556), ('forgettable', -1.0744630427075774), ('baldwin', -1.0889630415990892), ('refer', -1.1240466851052238), ('mst3k', -1.155794962798977), ('disappointment', -1.1714271923924522), ('210', -1.2547420230667088), ('310', -1.2978391307659871), ('410', -1.8871924506396707)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset stem_text preprocessed with CountVectorizer\n",
      "accuracy: 0.83064\n",
      "roc auc: 0.83064\n",
      "positive: \n",
      "[('710', 2.0195486641242395), ('driven', 1.1940194246771274), ('fritz', 1.0798944701687894), ('make bad', 1.0738679107218492), ('well worth', 1.0737335830537906), ('surprisingli good', 1.0588477745267701), ('kurosawa', 1.0278414701632579), ('never bore', 1.0204809468473193), ('erot', 1.0099167472105708), ('rather good', 1.00757953100431)]\n",
      "negative: \n",
      "[('tunnel', -1.039784550547438), ('unfunni', -1.0463465719145377), ('unwatch', -1.0764518441627766), ('brainwash', -1.098508459312289), ('210', -1.180443864328653), ('mst3k', -1.1954162590728552), ('poorli', -1.2144570040778773), ('baldwin', -1.411399344379379), ('310', -1.4621652921648027), ('410', -2.0242018061171723)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset lemm_text preprocessed with CountVectorizer\n",
      "accuracy: 0.8312\n",
      "roc auc: 0.8312\n",
      "positive: \n",
      "[('710', 1.9377918853667455), ('subsequently', 1.2212850658474443), ('810', 1.181138277324274), ('refresh', 1.1637980441755365), ('kurosawa', 1.1400099767326468), ('well worth', 1.1075367514460543), ('erotic', 1.0864238252526577), ('vengeance', 1.0588196053101235), ('excellently', 1.0161768273198737), ('well write', 0.9757144231214127)]\n",
      "negative: \n",
      "[('disappointment', -1.0518552012920899), ('two great', -1.067022598752175), ('skip one', -1.0820465319989823), ('lifeless', -1.1055405129476095), ('210', -1.1306555545514974), ('unfunny', -1.1395004593088367), ('poorly', -1.1427385908606194), ('310', -1.150844742717871), ('baldwin', -1.372492707671805), ('410', -2.0179114293740015)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='stem_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='lemm_text', count_vectorizer=CountVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим все те же операции, но теперь с TF-IDF вместо Bag-of-Words. Будем использовать сразу биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SGDClassifier on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88768\n",
      "roc auc: 0.88768\n",
      "positive: \n",
      "[('excellent', 3.6586176983394547), ('great', 3.564119687567475), ('perfect', 2.885532229361667), ('710', 2.8483684441392674), ('wonderful', 2.8036484400632617), ('best', 2.7408331783498254), ('amazing', 2.5992493743224943), ('favorite', 2.555094724974988), ('fun', 2.3540796983544943), ('today', 2.128088868744667)]\n",
      "negative: \n",
      "[('worse', -2.839250731208912), ('dull', -2.900881420763492), ('terrible', -2.916377481400749), ('nothing', -3.0432219121946837), ('waste', -3.4676889339485584), ('poor', -3.48932384321161), ('boring', -3.497366351256867), ('awful', -4.056783715704232), ('bad', -4.194764153509881), ('worst', -4.934198291219931)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88508\n",
      "roc auc: 0.88508\n",
      "positive: \n",
      "[('great', 3.766511227829611), ('excel', 3.7658930538468085), ('perfect', 2.991291215480247), ('enjoy', 2.8616445385365195), ('710', 2.7751865063117704), ('best', 2.7242618535680845), ('favorit', 2.573473179711535), ('fun', 2.4720009897226767), ('love', 2.459591167521324), ('amaz', 2.235280778862133)]\n",
      "negative: \n",
      "[('terribl', -2.902446627754427), ('fail', -3.0208758075468927), ('noth', -3.137488698000757), ('disappoint', -3.274221256492847), ('poor', -3.3569485661079526), ('bore', -3.831354054285834), ('bad', -3.9773816698859474), ('wast', -4.0756248806459165), ('aw', -4.2221609421830575), ('worst', -5.0026895790231025)]\n",
      "\n",
      "\n",
      "Model info: SGDClassifier on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.88568\n",
      "roc auc: 0.8856800000000001\n",
      "positive: \n",
      "[('great', 3.685355250449718), ('excellent', 3.6277120016155204), ('perfect', 2.876048516667139), ('710', 2.8081463934486774), ('wonderful', 2.7185550396295577), ('best', 2.6706552836737107), ('love', 2.4561830740796466), ('enjoy', 2.429315105762831), ('favorite', 2.4165880375997326), ('amaze', 2.354631280671017)]\n",
      "negative: \n",
      "[('unfortunately', -2.850626601572424), ('fail', -2.9758097921023805), ('dull', -3.0268544194345934), ('nothing', -3.127295234215515), ('poor', -3.352829358355799), ('boring', -3.44346015715892), ('waste', -4.021100521952159), ('awful', -4.061324049464304), ('bad', -4.593153074021695), ('worst', -5.07028375693627)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.85732\n",
      "roc auc: 0.8573199999999999\n",
      "positive: \n",
      "[('br', -5.16887109815529), ('film', -5.381540174913158), ('movie', -5.386409875387772), ('one', -5.809390239446535), ('great', -6.021980905099819), ('good', -6.078155162918667), ('like', -6.081240746738749), ('story', -6.144403918313189), ('see', -6.238909286990685), ('well', -6.269692941148184)]\n",
      "negative: \n",
      "[('total waste', -11.55785628346364), ('210', -11.564870457301069), ('br worst', -11.5748629440132), ('possibly worst', -11.578958367301635), ('thunderbirds', -11.579577292823947), ('acting horrible', -11.6390186700001), ('br 410', -11.6390186700001), ('hours life', -11.6390186700001), ('prom night', -11.6390186700001), ('worst acting', -11.6390186700001)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.85156\n",
      "roc auc: 0.85156\n",
      "positive: \n",
      "[('br', -5.211332643940145), ('film', -5.266744926204931), ('movi', -5.293740774290478), ('one', -5.818867037369389), ('like', -6.003675488038432), ('great', -6.053156311149506), ('good', -6.1048669754425955), ('stori', -6.110114483017613), ('love', -6.120881740798832), ('time', -6.142787479276566)]\n",
      "negative: \n",
      "[('br worst', -11.589621483877902), ('thunderbird', -11.591040321820499), ('ajay', -11.591856143511842), ('possibl worst', -11.592519351206873), ('act horribl', -11.649327418183676), ('br 410', -11.649327418183676), ('prom night', -11.649327418183676), ('slater', -11.649327418183676), ('wast film', -11.649327418183676), ('worst act', -11.649327418183676)]\n",
      "\n",
      "\n",
      "Model info: MultinomialNB on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.85484\n",
      "roc auc: 0.85484\n",
      "positive: \n",
      "[('br', -5.195927257985026), ('film', -5.251510655373332), ('movie', -5.282579338185142), ('one', -5.805030550347993), ('see', -5.8903591172238), ('like', -6.001023922788448), ('make', -6.0248249203147894), ('great', -6.039515247740439), ('story', -6.09702968813988), ('good', -6.098928700230642)]\n",
      "negative: \n",
      "[('gram', -11.578652332240145), ('br worst', -11.58380352019213), ('possibly worst', -11.587431773609012), ('thunderbird', -11.58816215992084), ('act horrible', -11.646378593156799), ('br 410', -11.646378593156799), ('prom night', -11.646378593156799), ('slater', -11.646378593156799), ('waste film', -11.646378593156799), ('worst act', -11.646378593156799)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.87428\n",
      "roc auc: 0.87428\n",
      "positive: \n",
      "[('710', 4.579413093665615), ('excellent', 3.073732287950895), ('perfect', 2.7399203965915295), ('well worth', 2.573228681784154), ('refreshing', 2.565564470544883), ('funniest', 2.4481363705393187), ('wonderful', 2.438888704840329), ('810', 2.4343710433790173), ('enjoyable', 2.3651312515000407), ('wonderfully', 2.3226718711212415)]\n",
      "negative: \n",
      "[('disappointing', -2.795391467101336), ('horrible', -2.8322437740245983), ('lacks', -2.865041427166171), ('310', -2.9041466014332986), ('poorly', -3.496523900173553), ('awful', -3.519776311567295), ('waste', -3.561312423536151), ('disappointment', -3.652942724324644), ('worst', -4.226693157044744), ('410', -4.289231517859529)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset stem_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.8722\n",
      "roc auc: 0.8722\n",
      "positive: \n",
      "[('710', 4.432595005481306), ('excel', 3.3224533188084555), ('perfect', 2.834573853437866), ('810', 2.7373774877005954), ('well worth', 2.6606579487174815), ('favorit', 2.471477304910311), ('refresh', 2.43715196027417), ('great', 2.3986031092259488), ('must see', 2.344788927195087), ('1010', 2.2227278413777967)]\n",
      "negative: \n",
      "[('horribl', -2.688348970775665), ('wors', -2.9379405107957095), ('wast', -3.0316602251254303), ('disappoint', -3.0591119394923876), ('310', -3.1046131179530425), ('bore', -3.2321966757836376), ('poorli', -3.6059389949729734), ('aw', -4.012956779808646), ('worst', -4.35571410487076), ('410', -4.370534535538874)]\n",
      "\n",
      "\n",
      "Model info: LinearSVC on dataset lemm_text preprocessed with TfidfVectorizer\n",
      "accuracy: 0.87152\n",
      "roc auc: 0.8715200000000001\n",
      "positive: \n",
      "[('710', 4.407966230683356), ('excellent', 3.1008458478353913), ('810', 2.9394189454038004), ('perfect', 2.8617773701022773), ('well worth', 2.6905141282777296), ('refresh', 2.498308910092633), ('wonderful', 2.455792327907999), ('favorite', 2.4487295622986154), ('great', 2.370397249310371), ('funniest', 2.3419106966960657)]\n",
      "negative: \n",
      "[('310', -2.8905632967791193), ('boring', -2.8929725500077064), ('horrible', -2.9037770460754673), ('bad', -2.9475422763183654), ('waste', -3.1036221364417145), ('disappointment', -3.343261670612485), ('poorly', -3.5065899159912357), ('awful', -3.8200490354833283), ('worst', -4.201175396533787), ('410', -4.279528199347008)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='stem_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=linear_model.SGDClassifier(), column='lemm_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='stem_text')\n",
    "train_and_predict_bow(estimator=MultinomialNB(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='lemm_text')\n",
    "\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='stem_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))\n",
    "train_and_predict_bow(estimator=svm.LinearSVC(), column='lemm_text', count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Использование TF-IDF дало прирост в точности примерно на 4%. Попробуем ещё настроить регуляризацию для SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_and_predict_grid(estimator=svm.SVC(), count_vectorizer=TfidfVectorizer(max_features=10000, ngram_range=(1,2)), column='text'):\n",
    "    print(\"Model info: \" + estimator.__class__.__name__ + \" on dataset \" + column + \" preprocessed with \" \n",
    "          + count_vectorizer.__class__.__name__)\n",
    "    \n",
    "    X_train = df[column]\n",
    "    y_train = df['target']\n",
    "    y_train = y_train.astype('bool')\n",
    "    X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[10, 100, 1000]}\n",
    "    grid = GridSearchCV(estimator, parameters, cv=3, n_jobs=-1, verbose=True)\n",
    "    grid.fit(X_train_bow, y_train)  \n",
    "    \n",
    "    \n",
    "    X_test_bow = count_vectorizer.transform(test_df[column])\n",
    "    y_test = test_df.target\n",
    "    y_test=y_test.astype('bool')\n",
    "\n",
    "    print('best parameters: ')\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    print(\"accuracy: \" + str(accuracy_score(y_test, grid.best_estimator_.predict(X_test_bow))))\n",
    "    print(\"roc auc: \" + str(roc_auc_score(y_test, grid.best_estimator_.predict(X_test_bow))))\n",
    "    print(\"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: SVC on dataset text preprocessed with TfidfVectorizer\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 119.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: \n",
      "{'C': 100, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'auto', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "accuracy: 0.8586\n",
      "roc auc: 0.8586\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_grid(estimator=svm.SVC(), column='text', count_vectorizer=TfidfVectorizer(max_features=1000, ngram_range=(1,2)))\n",
    "# train_and_predict_grid(estimator=svm.SVC(), column='stem_text', count_vectorizer=TfidfVectorizer(max_features=1000, ngram_range=(1,2)))\n",
    "# train_and_predict_grid(estimator=svm.SVC(), column='lemm_text', count_vectorizer=TfidfVectorizer(max_features=1000, ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соседнем ноутбуке все те же операции выполнены с фильтраций числовых символов.\n",
    "Из общих итогов:\n",
    "- tf-idf показал себя в задаче лучше чем bag-of-words. Общий прирост по обоим метрикам - 4%.\n",
    "- при использование ngram=1 стеминг > лемматизация > простой текст. Однако, при использовании биграм и триграм простой текст начал показывать такой же или чуть лучший результат.\n",
    "- из моделей лучшие показатели выглядят с небольшой разницей так: SGD > SVM > NB. Но NB и SGD показали гораздо более высокую скорость обучения.\n",
    "- выключение числовых данных привело к небольшому ухудшению ~0-1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
